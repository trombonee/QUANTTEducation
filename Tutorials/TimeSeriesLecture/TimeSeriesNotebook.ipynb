{
 "cells": [
  {
   "source": [
    "# **Time Series Analysis**\n",
    "## Some Terminology\n",
    "- Equities/Stocks: Shares of ownership issued by publicly traded companies (What we will be trading)\n",
    "- Ticker: The symbol used to represent a publicly traded company (ex. AAPL = Apple, TSLA = Tesla, etc.)\n",
    "## What is a time series?\n",
    "In many fields of study data is collected from a system. The obserbations made of such a process will generate a time series. In simplest terms, a time series is a set of data indexed by time. For the purposes of quant finance, we will be working with financial time series such as the daily closing price of a stock or the quarterly profits of a company. A great python module for working with this type of data is Pandas, which will be covered later on.\n",
    "### Examples of time series:\n",
    "Below is an example of how a time series that we will be working with is structured. Notice how the indexes are strictly increasing with constant time intervals\n",
    "\n",
    "![Image of SPY Time Series](https://raw.githubusercontent.com/trombonee/QUANTTEducation/master/Tutorials/TimeSeriesLecture/img/SPYTimeSeries.png)\n",
    "\n",
    "We can also plot time series data to get a better visualization of possible trends or correlations in data\n",
    "\n",
    "![Image of SPY Time Series Plot](https://raw.githubusercontent.com/trombonee/QUANTTEducation/master/Tutorials/TimeSeriesLecture/img/SPYTimeSeriesPlot.png)\n",
    "\n",
    "## The above can be accomplished with Python and a few modules\n",
    "\n",
    "### Step One: Import necessary modules\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Step Two: Load time series data set (Data in QuantConnect will be loaded differently)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  open    high      low   close    volume\n",
       "name date                                                \n",
       "AAPL 9/12/2017  162.61  163.96  158.770  160.86  71714046\n",
       "     9/13/2017  159.87  159.96  157.910  159.65  44907361\n",
       "     9/14/2017  158.99  159.40  158.090  158.28  23760749\n",
       "     9/15/2017  158.47  160.97  158.000  159.88  49114602\n",
       "     9/18/2017  160.11  160.50  157.995  158.67  28269435"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>name</th>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">AAPL</th>\n      <th>9/12/2017</th>\n      <td>162.61</td>\n      <td>163.96</td>\n      <td>158.770</td>\n      <td>160.86</td>\n      <td>71714046</td>\n    </tr>\n    <tr>\n      <th>9/13/2017</th>\n      <td>159.87</td>\n      <td>159.96</td>\n      <td>157.910</td>\n      <td>159.65</td>\n      <td>44907361</td>\n    </tr>\n    <tr>\n      <th>9/14/2017</th>\n      <td>158.99</td>\n      <td>159.40</td>\n      <td>158.090</td>\n      <td>158.28</td>\n      <td>23760749</td>\n    </tr>\n    <tr>\n      <th>9/15/2017</th>\n      <td>158.47</td>\n      <td>160.97</td>\n      <td>158.000</td>\n      <td>159.88</td>\n      <td>49114602</td>\n    </tr>\n    <tr>\n      <th>9/18/2017</th>\n      <td>160.11</td>\n      <td>160.50</td>\n      <td>157.995</td>\n      <td>158.67</td>\n      <td>28269435</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/trombonee/QUANTTEducation/master/Tutorials/TimeSeriesLecture/AAPL_data.csv', index_col=['name', 'date'])"
   ]
  },
  {
   "source": [
    "### Step Three: Plot the time series data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Motivation: What if there was a way to perform analysis on this data to forecast future values?\n",
    "Clearly if we were able to predict the future values of an equity, there would be a potential to make extremely large amounts of money. Even if we could predict correctly 51% of the time, this could be leveraged over a large number of trades to make significant profits. The study of time series analysis attempts to apply math methods and statistics to this problem.\n",
    "\n",
    "## How do we go about doing this?\n",
    "Modern time series analysis dates back to 1927 when a statitician G. U. Yule used an **autoregressive model** to describe the dynamic movement of a pendulum. As who have taken a course on differential equations may know, the movement of a pendulum can be approximated by a second order linear differential equation. Differential equations are used to describe dynamic processes in continuous time, but time series data is typically sampled in discrete time steps (i.e., the price of stock every minute, every hour or every day). If the second order differential equation of a pendulum is substituted with discrete time differences, the resulting equation takes on the form:\n",
    "\n",
    "![Image discrete pendulum equation](https://raw.githubusercontent.com/trombonee/QUANTTEducation/master/Tutorials/TimeSeriesLecture/img/DiscretePendulum.png)\n",
    "\n",
    "The important thing to note here is that the position at time **t** relies on the position of the pendulum at time **t-1** and **t-2**. This is called an **autoregressive model** specifically because of this behaviour. Our goal is to apply similar ideas to create models for financial time series and forecast useful information.\n",
    "\n",
    "**NOTE:** Notice, there are coefficient terms $\\phi_1$ and $\\phi_2$. If these values are too large, say +2 and +3, this equation will go to $|\\infty|$ as t goes to $+\\infty$. The need to find coefficients such that the equation remains stable (finite valued) brings rise to the topic of stationarity. \n",
    "\n",
    "# Stationarity:\n",
    "## What is stationarity?\n",
    "A time series is stationary if it is a finite variance process of which the mean and variance are constant in time (i.e., $E[z_t]=\\mu$ and $Var[z_t]=\\sigma^2$ for all time **t**), and the correlation between observations at different points in time is only lag dependent (i.e., $Cov(z_{t+k},z_t)$ depends only on k for all of time **t**)\n",
    "\n",
    "We will never specifically need to compute these values by hand so we will not cover their formulas, but feel free to do your own research. These computations can however be made quite easily with the NumPy module in Python\n",
    "\n",
    "### Step One: Import NumPy\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Step Two: Peform expected value computation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Step Three: Perform variance calculation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Step Four: Perform auto-covariance calculation\n",
    "This calculation is a little more involved since it looks at two seperate time sets **T** offset by some integer **k**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Sometimes it is possible to predict if a time series is stationary based off of its graph. Can you tell which graphs below show the characteristics of stationarity?\n",
    "Hint: Remember we are looking for finite mean and finite variance\n",
    "\n",
    "![Image of Stationarity Test](https://raw.githubusercontent.com/trombonee/QUANTTEducation/master/Tutorials/TimeSeriesLecture/img/StationarityTest.png)\n",
    "\n",
    "## Why do we care about stationarity?\n",
    "The Wold Decomposition Theorem states that any stationary time series can be decomposed and represented in the following form:\n",
    "\n",
    "# $z_t=a_t+\\sum^\\infty_{j=1}\\psi_ja_{t-j}+\\mu$\n",
    "\n",
    "Where $a_t$'s are uncorrelated random shocks with zero mean and constant variance (i.e., white noise), $\\psi_j$ satisfies $\\sum_{i=0}^\\infty\\psi_i^2<\\infty$ and $\\mu$ is the mean of the time series\n",
    "\n",
    "The forecasting methods we will cover essentially provide ways to approximate the values to the above equation, which is why having stationarity in our time series is so important.\n",
    "\n",
    "## What if our time series is not stationary?\n",
    "### Data Transformations\n",
    "Transformations on time series data are sometime necessary when dealing with nonlinear data. It could make sense to scale the data in a way such that it becomes linear. A common example of this is the Richter scale which uses a log base. Often the main goal of a transformation is to make the variance in our data constant. One useful transformation when working with financial data could be the percentage change.\n",
    "\n",
    "# $p_t=\\frac{z_{t}-z_{t-1}}{z_{t-1}}$\n",
    "\n",
    "### Apply the percentage change transformation to the AAPL data:\n",
    "#### Original Plot:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "#### With Percentage Change:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Notice how the time series immediately looks to be more stationary\n",
    "\n",
    "### Differencing\n",
    "Differencing provides another way modify the dataset in hopes of acheiving stationarity. The first order difference is the easiest to understand. If considering the daily price values of an equity, the first order differenced time series would be the daily change in price time series. Formally, this is written as:\n",
    "\n",
    "# $\\Delta z_t=z_t-z_{t-1}$\n",
    "\n",
    "It can be accomplished very easily in Python\n",
    "\n",
    "#### First order difference of AAPL:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "**NOTE:** Notice how this looks very similar to the percent change plot on a larger scale\n",
    "\n",
    "If a first order differenced time series is not yet stationary, we can apply the differecing process again:\n",
    "\n",
    "# $\\Delta^2z_t=\\Delta (z_t-z_{t-1})=(z_t-z_{t-1}-z_{t-1}-z_{t-2})=z_t-2z_{t-1}-z_{t-2}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "**NOTE:** This process can be repeated as many times as necessary to achieve stationarity, however in practice it is almost never required to go beyond second-order differencing. An important thing to keep in mind is that each time a time series is differenced, the series loses \"memory\" in the data (i.e., trends, seasonality, etc.). This can cause issues when trying to use the data in forecasting, hence it is important to apply the minimum number of differences possible\n",
    "\n",
    "Now that we know how to transform and difference our data we must be able to check for stationarity\n",
    "\n",
    "## How do we check for stationarity?\n",
    "The most simple method of checking for stationarity would involve visual analysis as we did in the example earlier, however this is not something that could be easily implemented with code. Luckily there have been some genius mathematicians that have developed algorithms which can predict if a time series is stationary. One of the more common ones used in industry is known as the Augmented Dickey-Fuller test. This tests the null hypothesis that a **unit root** is present in a time series sample (i.e., 1 is a root in the characteristic equation for the time series, assuming the series can be represented by a characteristic equation). We are simply looking for this hypothesis to be rejected meaning there is no **unit root**.\n",
    "\n",
    "One Python module that implements this algorithm is **statsmodels**, where the function returns an array including a p-value. Typically if $p < 5\\%$ we can reject the null hypothesis and assume that the time series is staitionary.\n",
    "\n",
    "### Check if AAPL is stationary:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "source": [
    "Not surprisingly, the time series is non-stationary. Now what if we difference it?\n",
    "\n",
    "### Difference AAPL until it is stationary:\n",
    "**NOTE:** When you difference a time series, the first value will become NaN. This must be dealt with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Even still the time series is not stationary so we must difference once again:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Finally, we have found a p-value less than 0.05 so we can call the time series stationary. However since we had to difference the time series twice we have lost a significant amount of \"memory\" from the original time series.\n",
    "\n",
    "Now that we can detect stationarity, lets discuss some forecasting methods\n",
    "\n",
    "## Forecasting Methods\n",
    "We will not dive into the mathematics behind these methods but rather step through the process of using them\n",
    "\n",
    "### AR (Autoregression) Model\n",
    "An Autoregression model takes on the following form with parameter $p$:\n",
    "\n",
    "# $z_t=\\phi_1z_{t-1}+\\phi_2z_{t-2}+...+\\phi_pz_{t-p}+a_t$\n",
    "\n",
    "Where $a_t$ is an error term, $\\phi_i$'s are coefficients estimated from data and $z_t$ is the value of the time series at time t \n",
    "\n",
    "### MA (Moving Average) Model\n",
    "Theses models are \"averages\" of past and present error terms. A Moving Average model takes on the following form with parameter $q$:\n",
    "\n",
    "# $z_t=a_t-\\theta_1a_{t-1}-...-\\theta_qa_{t-q}$\n",
    "\n",
    "Where $a_t$'s are once again error terms, $\\theta_i$'s are coefficients estimated from data and $z_t$ is the value of the time series at time t\n",
    "\n",
    "You will rarely implement either of these models on their own. The AR(p) and MA(q) models are often used as the building blocks of more sophisticated models in time series analysis\n",
    "### ARMA (Autoregressive Moving Average) Model\n",
    "\n",
    "Not surprisingly this model was developed by combining autoregression and moving average models. These models take on the following form with parameters $p$ and $q$:\n",
    "\n",
    "# $z_t=\\phi_1z_{t-1}+\\phi_2z_{t-2}+...+\\phi_pz_{t-p}+a_t-\\theta_1a_{t-1}-...-\\theta_qa_{t-q}$\n",
    "\n",
    "This model is very common when modeling stationary data. Both $p$ and $q$ represent a lag period for their respective model, it is up to the user to determine the optimal value for each.\n",
    "\n",
    "### ARIMA (Autoregressive Integrated Moving Average) Model\n",
    "The ARIMA model takes ARMA one step further by adding a parameter $d$, which is the number of times we want to difference our time series before applying the ARMA model to it. This allows us to input non-stationary data into the model. We will now attempt to fit an **ARIMA** model with Python.\n",
    "\n",
    "### Step One: Import the necessary Modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.ar_model import ar_select_order\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "### Step Two: Inspect Auto-Correlation and Partial Auto-Correlation Plots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This gives us some insight into how auto-correlated our data is. On the bottom axis is the lag period (i.e., when $z_t$ is compared to $z_{t-1}$ this represents a lag of 1). Based on these graphs we would not want to use a lag value greater than two.\n",
    "\n",
    "### Step Three: Determine value for p"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Sometimes an array of multiple lag values will be returned. You are able to input the entire array in the ARIMA model as parameter $p$.\n",
    "### Step Four: Forecast with model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Using this forecasted result you could then implement logic in your algorithm to make a trading decision.\n",
    "\n",
    "### Below is an example of how an ARIMA model would forecast the AAPL data using a 30 day lookback period"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing = closing_prices.values\n",
    "forecast = [np.nan]*30\n",
    "for i in range(30, len(closing)):\n",
    "    window = closing[i-30:i]\n",
    "    model = ARIMA(window, order=(1,2,0))\n",
    "    result = model.fit()\n",
    "    forecast.append(result.forecast()[0])\n",
    "\n",
    "df['forecast'] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc['AAPL'][['close','forecast']]\n",
    "data.plot()"
   ]
  },
  {
   "source": [
    "This **ARIMA** model in the **statsmodels** library is very powerful as it also allows you to account for seasonality in your data with a few extra parameters. I would highly recommend doing more research into it. If you do plan to use this in an algorithm, you will see the best results if you can find data sets which exhibit properties of stationarity and have high auto-correlation. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0e0ef913279acd06d4c7bdd7f1f19dda6c2ffef9bae0beef6d818d84fdd005006",
   "display_name": "Python 3.8.5 32-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e0ef913279acd06d4c7bdd7f1f19dda6c2ffef9bae0beef6d818d84fdd005006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}